#!/bin/python3

###############################################################
#                   University of Pretoria                    #
#               COS 314 - Artificial Intelligence             #
#                 Assignment 2 - Decision Trees               #
#                                                             #
#                   Regan Koopmans, 15043143                  #
#                                                             #
###############################################################


import argparse
import os.path
import math
import random
import copy

# Main function. Invokes helper functions defined below to perform tree induction
# from given files.

def main():
    parse_args()
    parse_spec_file(args.spec)
    parse_data_file(args.dat)
    partition_sets()
    induce(training_set, None, [], {})
    if flags["prune_tree"]:
        prune()
    print("Classify on training\t=> " + str(test_tree(training_set, rules)) + "% accuracy")
    print("Classify on testing\t=> " + str(test_tree(testing_set, rules)) + "% accuracy")
    output_file.close()


# List containing the possible classifications in the system.
classes = []

# List containing the possible entries of a data field in the dataset.
attributes = []

# Dictionary of flags to modify behaviour
flags = { 'use_continuous' : False,
          'use_missing_data' : False,
          'prune_tree' : False }
args = {}

# Data structure to hold the training set in memory.
training_set = []

# Data structure to hold the testing set in memory.
testing_set = []

# Data structure to hold the rules generated by the induction algorithm.
rules = []

# Global file pointer to output rules to.
output_file = open("data.out","w")

# Helper function to print an error message and exit the program.

def fatalError(msg):
    print()
    print("Error: " + msg)
    print()
    os._exit(0)

# Function that will extract an array from a string with items in
# "{ item, item, item, ..., item  }"
# format

def extract_array(input):
    input = input.replace('{','')
    input = input.replace('}','')
    array = list(map(lambda x : x.strip(), input.strip().split(",")))
    return array

# Manages the parsing of command line arguments and populating the
# correct flags as per those given arguments.

def parse_args():
    global args
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", help="Use only discrete values.",
                        action="store_true")

    parser.add_argument("-c", help="Use both continuous and discrete values.",
                        action="store_true")

    parser.add_argument("-md", help="Use only discrete values, permit missing "
                        + "information.", action="store_true")

    parser.add_argument("-mc", help="Use continuous and discrete values, "
                        + "permit missing information.", action="store_true")

    parser.add_argument("-pd", help="Use only discreet values, prune decision "
                        + "tree.", action="store_true")

    parser.add_argument("-pc", help="Use continuous and discrete values, prune"
                        + " decision tree.", action="store_true")

    parser.add_argument("spec")
    parser.add_argument("dat")
    args = parser.parse_args()

    # Set program flags for behaviour

    if args.c or args.mc or args.pc:
        flags['use_continuous'] = True
    if args.md or args.mc:
        flags['use_missing_data'] = True
    if args.pd or args.pc:
        flags['prune_tree'] = True

# Populates attributes from the .spec file

def parse_spec_file(file_name):
    global classes
    if os.path.exists(file_name):
        file = open(file_name,  "r")
        read_classname = False;
        for line in file:
            line_array = list(map(lambda x : x.strip(), line.strip().split(":")))
            if not read_classname:
                classes = line_array[1].split(" ")
                read_classname = True
            else:
                new_attrib = {}
                new_attrib["name"] = line_array[0]
                if line_array[1] == 'Real':
                    if flags['use_continuous']:
                        new_attrib["type"] = line_array[1]
                    else:
                        fatalError("Continuous attribute found. Please run with a flag that permits continuous data.")
                else :
                    new_attrib["type"] = extract_array(line_array[1])
                attributes.append(new_attrib)
        file.close()
    else:
        fatalError("Specification file not found!")

# Adds an entry to the training set from a line in the data file.
# Checks for correct input, and converts to a real number where appropriate.

def add_entry(line):
    line = line.strip()
    line_array = list(map(lambda x : x.strip(), line.split(" ")))
    new_entry = {}
    for x in range(0,len(attributes)):
        if line_array[x] == '?':
            if flags['use_missing_data']:
                new_entry[attributes[x]['name']] = None;
            else:
                fatalError("Missing data encountered. Please run with a flag that permits missing data.")
        elif attributes[x]["type"] == 'Real':
            new_entry[attributes[x]['name']] = float(line_array[x])
        else:
            new_entry[attributes[x]['name']] = line_array[x]
            if line_array[x] not in attributes[x]["type"]:
                fatalError("Unexpected value \"" + line_array[x] + "\" for field " + attributes[x]['name']
                           + " in data file. Expected " + str(attributes[x]['type']))
    
    new_entry['class'] = line_array[-1]
    training_set.append(new_entry)
    return

# Iterates through the given data file, add entries per line.

def parse_data_file(file_name):
    if os.path.exists(file_name):
        file = open(file_name,  "r")
        data = datalines = (line.rstrip('\r\n') for line in file);
        line_count = 1
        for line in data:
            add_entry(line)
        file.close()
    else:
        fatalError("Data file not found!")

# Splits training set and testing set 70 / 30

def partition_sets():
    global training_set
    global testing_set

    # Randomize set to ensure that the input set is not partially
    # or totally ordered already.

    random.shuffle(training_set)
    length = len(training_set)
    testing_set = training_set[:int(0.3*length)]
    training_set = training_set[int(0.3*length):]

# Returns a subset of the input set, where all the entries have
# the value `test_value` for attribute `test_attribute`

def subset(input_set, test_attribute, test_value):
    new_set = []
    for entry in input_set:
       if entry[test_attribute] == test_value:
           new_set.append(entry)
    return new_set

# Function to calculate the entropy of a set where there are `num_true`
# amount of positive results in a global set of size `total`

def entropy(num_true, total):
    if num_true == 0 or total == 0:
        return -0
    return num_true/total * math.log(num_true/total,2)

def entropy_of_set(input_set):
    if len(input_set) == 0:
        return 0.0
    entropy_total = 0
    class_totals = {}

    # set counters of classes to zero
    for entry_class in classes:
        class_totals[entry_class] = 0
    for entry in input_set:
        for entry_class in classes:
          if entry['class'] == entry_class:
             class_totals[entry_class] += 1

    # calculate the net entropy by summing over the entropy of the classes

    for entry_class in classes:
        entropy_total += entropy(class_totals[entry_class], len(input_set))
    entropy_total = abs(entropy_total)
    return entropy_total

def probability_of_missing(input_set, attribute):
    missing_count = 0
    for entry in input_set:
        if entry[attribute["name"]] == None:
           missing_count += 1
    return missing_count / len(input_set)

def frequency(input_set, attribute, value):
    count = 0
    for entry in input_set:
        if entry[attribute] == value:
            count += 1
    return count / len(input_set)

# A summation of the entropies of each value valid for a certain
# given attribute

def average_entropy(input_set, attribute):
    total = 0
    for value in attribute["type"]:
         total += frequency(input_set, attribute["name"], value) * entropy_of_set(subset(input_set, attribute["name"],value))
    return total

# Function that calculates the gain of partitioning a set
# with criterion test.

def gain(input_set, attribute):
    return (1-probability_of_missing(input_set, attribute))*(entropy_of_set(input_set) - average_entropy(input_set, attribute))

# Splits an input set into subsets based on value

def split_set(input_set, attribute, continuous_descriminator, debug):
    set_array = []
    sets = {}
    if attribute == None:
        return

    if attribute["type"] != 'Real':
        for value in attribute["type"]:
            sets[value] = []

        # For every entry in the set. If the entries attribute is not none,
        # add it to the set of those attributes, otherwise propogate to all sets.

        for entry in input_set:
            if entry[attribute["name"]] != None:
                sets[entry[attribute["name"]]].append(entry)
        for value in attribute["type"]:
            if len(sets[value]) > 0:
                set_array.append(sets[value])
    else:
        greater = []
        less = []
        for entry in input_set:
            if entry[attribute["name"]] > continuous_descriminator:
                greater.append(entry)
            else:
                less.append(entry)

        set_array.append(greater)
        set_array.append(less)
    return set_array

# Generates rule components from splits described using string

def parts_from_splits(splits):
    parts = []
    for entry in splits:
        if " < " in entry:
            line_array = entry.split(" < ")
            parts.append((line_array[0], float(line_array[1]), "<"))
        elif " > " in entry:
            line_array = entry.split(" > ")
            parts.append((line_array[0], float(line_array[1]), ">"))
        else :
            line_array = entry.split(" = ")
            parts.append((line_array[0], line_array[1], "="))
    return parts

# Retrieves the first non-missing element of an attriubte in a given
# input set

def attribute_value(input_set, attribute):
    for entry in input_set:
        if entry[attribute["name"]] != None:
            return entry[attribute["name"]]
    return None

# Function that checks whether a set has any difference other than classifications

def induceable(input_set, forbidden_attributes):
    if len(input_set) == 0:
        return False

    if forbidden_attributes != None:
        if len(forbidden_attributes) == len(attributes):
            return False

    for attribute in attributes:
        attribute_value = input_set[0][attribute["name"]]
        for entry in input_set:
            if entry[attribute["name"]] != attribute_value:
                return True;
    return False

# Populates a dictionary of the continuous values seen in the given input set,
# useful for calculating gain and splitting.

def populate_cont_attribute_values(input_set):
    cont_attribute_values = {}
    for attribute in attributes:
        if attribute["type"] == "Real":
            cont_attribute_values[attribute["name"]] = []

    for attribute in attributes:
        if attribute["type"] == "Real":
            for entry in input_set:
                if entry[attribute["name"]] not in cont_attribute_values[attribute["name"]]:
                    cont_attribute_values[attribute["name"]].append(entry[attribute["name"]])
    # Sorting

    for key in cont_attribute_values.keys():
        cont_attribute_values[key].sort()
    return cont_attribute_values


# Calculates the gain for splitting on some continuous attribute, and returns that gain
# and the continuous discriminator used.

def gain_continuous(input_set, attribute, attribute_values, forbidden_attribute_reals):
    gain = (0,0)
    if attribute_values is None:
        return (0, None)
    set_entropy = entropy_of_set(input_set)
    max_continuous_gain = -1
    max_continuous_descriminator = -1
    for x in range(0, len(attribute_values)-1):
        current_descriminator =(attribute_values[x] + attribute_values[x+1]) / 2
        if current_descriminator not in forbidden_attribute_reals[attribute["name"]]:
            sets = split_set(input_set, attribute, current_descriminator, False)
            current_gain = set_entropy - (len(sets[0])/len(input_set) * entropy_of_set(sets[0]) + len(sets[1])/len(input_set) * entropy_of_set(sets[1]))
            if current_gain > max_continuous_gain:
                max_continuous_gain = current_gain
                max_continuous_descriminator = current_descriminator
    gain = (max_continuous_gain, max_continuous_descriminator)
    return gain

# Function that encapsulates the main logic of constructing the decision tree.

# input_set                 - the set to be induced.
# splits_performed          - a set of splits that have been performed prior (ie. inheritted information).
# forbidden_attributes      - a set of attributes that have already been split on, and cannot be used again (discrete).
# forbidden_attribute_reals - a set of continuous splits for particular attributes (cannot split on the same 
#                              attribute, at the same place twice).

def induce(input_set, splits_performed, forbidden_attributes, forbidden_attribute_reals):
    if len(input_set) == 0:
        return
    set_entropy = entropy_of_set(input_set)
    if set_entropy != 0.0:
        if not induceable(input_set, forbidden_attributes):
            return

        continuous_attributes = populate_cont_attribute_values(input_set);
        continuous_descriminator = None
        cont_gain = (None,None)
        max_gain_attribute = None
        max_gain = -1
        forbidden_attributes = copy.deepcopy(forbidden_attributes)
        for attribute in attributes:
            if attribute["name"] not in forbidden_attributes:
                if attribute["type"] != "Real":
                    attribute_gain = gain(input_set, attribute)
                else:
                    if attribute["name"] not in forbidden_attribute_reals:
                        forbidden_attribute_reals[attribute["name"]] = []
                    if continuous_descriminator not in forbidden_attribute_reals[attribute["name"]]:
                        cont_gain = gain_continuous(input_set, attribute, continuous_attributes[attribute["name"]], forbidden_attribute_reals)
                        attribute_gain = cont_gain[0]

                if attribute_gain > max_gain:
                    max_gain = attribute_gain
                    max_gain_attribute = attribute
                    continuous_descriminator = cont_gain[1]

        if max_gain_attribute == None:
            return
        children_sets = split_set(input_set, max_gain_attribute, continuous_descriminator, True)
        if (max_gain_attribute["type"] != "Real"):
            forbidden_attributes.append(max_gain_attribute["name"])
        else:
            if forbidden_attribute_reals[max_gain_attribute["name"]] == None:
                forbidden_attribute_reals[max_gain_attribute["name"]] = []

            if continuous_descriminator not in forbidden_attribute_reals[max_gain_attribute["name"]]:
                forbidden_attribute_reals[max_gain_attribute["name"]].append(continuous_descriminator)

        for child in children_sets:
            if len(child) > 0:
                child_splits_performed = [] if splits_performed == None else splits_performed[:]
                if max_gain_attribute["type"] != "Real":
                    child_splits_performed.append(max_gain_attribute["name"] + " = " + attribute_value(child,max_gain_attribute))
                else:
                    example_value = attribute_value(child,max_gain_attribute)
                    if example_value < continuous_descriminator:
                        child_splits_performed.append(max_gain_attribute["name"] + " < " + str(continuous_descriminator))
                    else:
                        child_splits_performed.append(max_gain_attribute["name"] + " > " + str(continuous_descriminator))

                induce(child,child_splits_performed, forbidden_attributes, forbidden_attribute_reals)
    else:
        if splits_performed != None and len(splits_performed) > 0:
            output_string = "IF "
            for x in range(0,len(splits_performed)):
                output_string += "(" + splits_performed[x] + ")"
                if x < len(splits_performed) - 1:
                   output_string += " AND "
                rule = {}
                rule["class"] = input_set[0]["class"]
                rule["parts"] = parts_from_splits(splits_performed)
                if rule not in rules:
                    rules.append(rule)
            output_string += " THEN\n"
            output_string += "  CLASS IS " + input_set[0]["class"] + "\n\n"
            print(output_string)
            output_file.write(output_string)

# Helper function to determine whether a given entry has any missing values.

def has_missing_data(entry):
    for key in entry.keys():
        if entry[key] == None:
            return True
    return False

# Given an array of classfications (`classes`), this function returns
# the most commonly occuring class.

def max_class(classes):
    if (len(classes) == 1):
        return classes[0]
    max = -1
    st = set(classes)
    mx = -1
    h = None
    for each in st:
        temp = classes.count(each)
        if mx < temp:
            mx = temp
            h = each
    return h

# Returns a classification for an entry, using some set of rules 
# (rules are usually the global `rules`, but augmented rules are 
# used to check the validity of pruning).

# For an entry with missing values, a collection of classifcations
# is gathered, and then the maximal occuring classification is 
# returned.

def classify(entry, using_rules):
    entry_has_missing_data = has_missing_data(entry)
    applied_classes = []
    for rule in using_rules:
        passed_rule = True
        for part in rule["parts"]:
            if part[2] == "=":
                if entry[part[0]] != part[1]:
                    passed_rule = False
            elif part[2] == ">":
                if entry[part[0]] < part[1]:
                    passed_rule = False
            elif part[2] == "<":
                if entry[part[0]] > part[1]:
                    passed_rule = False
        if passed_rule == True:
            applied_classes.append(rule["class"])

    if flags["use_missing_data"]:
        return max_class(applied_classes)
    else:
        if len(applied_classes) > 0:
            return applied_classes[0]

# Function that tests the rules in the rule array against
# some training set

def test_tree(testing_set, using_rules):
    if len(testing_set) == 0:
       return
    correct_count = 0
    for entry in testing_set:
        if entry["class"] == classify(entry, using_rules):
            correct_count += 1
    return round(100 *(correct_count / len(testing_set)))


# Function to perform pruning. Pruning is attempted on the most complex
# rules in the `rules` data structure. 

def prune():
    max_rule_length = -1
    augmented_rules = []
    global rules
    for rule in rules:
        rule_length = len(rule["parts"])
        if rule_length > max_rule_length:
            max_rule_length = rule_length
    for rule in rules:
        if len(rule["parts"]) != max_rule_length:
            augmented_rules.append(rule)
        else:
            augmented_rule = copy.deepcopy(rule)
            augmented_rule["parts"].pop()
            augmented_rules.append(augmented_rule)

    # if the accuracy loss is smaller than 5, apply changes to master copy of
    # the rules

    if test_tree(training_set, rules) - test_tree(training_set, augmented_rules) <= 5:
        rules = copy.deepcopy(augmented_rules)
        print("Pruning was applied.")


# Calls the main function when program is run from command line. 

if __name__ == '__main__':
    main()