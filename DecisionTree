#!/bin/python3

import argparse
import os.path
import math
import random

# List containing the possible classifications in the system.

classes = []

# List containing the possible entries of a data field in the dataset.

attributes = []

# Dictionary of flags to modify behaviour

flags = { 'use_continuous' : False,
          'use_missing_data' : False,
          'prune_tree' : False }
args = {}

training_set = []
testing_set = []

# Helper function to print an error message and exit the program.

def fatalError(msg):
    print("Error: " + msg)
    os._exit(0)

# Function that will extract an array from a string with items in
# "{ item, item, item, ..., item  }"
# format

def extract_array(input):
    input = input.replace('{','')
    input = input.replace('}','')
    array = list(map(lambda x : x.strip(), input.strip().split(",")))
    return array

# Manages the parsing of command line arguments and populating the
# correct flags as per those given arguments.

def parse_args():
    global args
    parser = argparse.ArgumentParser()
    parser.add_argument("-d", help="Use only discrete values.",
                        action="store_true")

    parser.add_argument("-c", help="Use both continuous and discrete values.",
                        action="store_true")

    parser.add_argument("-md", help="Use only discrete values, permit missing "
                        + "information.", action="store_true")

    parser.add_argument("-mc", help="Use continuous and discrete values, "
                        + "permit missing information.", action="store_true")

    parser.add_argument("-pd", help="Use only discreet values, prune decision "
                        + "tree.", action="store_true")

    parser.add_argument("-pc", help="Use continuous and discrete values, prune"
                        + " decision tree.", action="store_true")

    parser.add_argument("spec")
    parser.add_argument("dat")
    args = parser.parse_args()

    # Set program flags for behaviour

    if args.c or args.mc or args.pc:
        flags['use_continuous'] = True

    if args.md or args.mc:
        flags['use_missing_data'] = True

    if args.pd or args.pc:
        flags['prune_tree'] = True

# Populates attributes from the .spec file

def parse_spec_file(file_name):
    print("Parsing " + file_name)
    global classes
    if os.path.exists(file_name):
        file = open(file_name,  "r")
        read_classname = False;
        for line in file:
            line_array = list(map(lambda x : x.strip(), line.strip().split(":")))
            if not read_classname:
                classes = line_array[1].split(" ")
                read_classname = True
            else:
                new_attrib = {}
                new_attrib["name"] = line_array[0]
                if line_array[1] == 'Real':
                    if flags['use_continuous']:
                        new_attrib["Type"] = line_array[1]
                    else:
                        fatalError("Continuous attribute found. Please run with a flag that permits continuous data.")
                else :
                    new_attrib["Type"] = extract_array(line_array[1])
                attributes.append(new_attrib)
        file.close()
    else:
        fatalError("Specification file not found!")

# Adds an entry to the training set from a line in the data file.
# Checks for correct input, and converts to a real number where appropriate.

def add_entry(line):
    line = line.strip()
    line_array = list(map(lambda x : x.strip(), line.split(",")))
    new_entry = {}
    for x in range(0,len(attributes)):
        if line_array[x] == '?':
            if flags['use_missing_data']:
                new_entry[attributes[x]['name']] = None;
            else:
                fatalError("Missing data encountered. Please run with a flag that permits missing data.")
        elif attributes[x]['Type'] == 'Real':
            new_entry[attributes[x]['name']] = int(line_array[x])
        else:
            new_entry[attributes[x]['name']] = line_array[x]
            if line_array[x] not in attributes[x]['Type']:
                fatalError("Unexpected value \"" + line_array[x] + "\" for field " + attributes[x]['name']
                           + " in data file. Expected " + str(attributes[x]['Type']))
    	
        new_entry['class'] = line_array[-1]
        training_set.append(new_entry)

def parse_data_file(file_name):
    print("Parsing " + file_name)
    if os.path.exists(file_name):
        file = open(file_name,  "r")
        for line in file:
            add_entry(line)
        file.close()
    else:
        fatalError("Data file not found!")

# Splits training set and testing set 70 / 30

def partition_sets():
    global training_set
    global testing_set
    random.shuffle(training_set)
    length = len(training_set)
    testing_set = training_set[:int(0.3*length)]
    training_set = training_set[int(0.3*length):]
    #print()
    #print("Training set : " + str(training_set))
    #print()
    #print("Testing set : " + str(testing_set))
    #print()

def write_output():
    file = open("data.out", "w")
    file.write("TODO")
    file.write("\n")
    file.close()

# Returns a subset of the input set, where all the entries have
# the value `test_value` for attribute `test_attribute`

def subset(input_set, test_attribute, test_value):
    new_set = []
    for entry in input_set:
       if entry[test_attribute] == test_value:
           new_set.append(entry)
    return new_set

# Function to calculate the entropy of a set where

def entropy(num_true, total):
    if num_true == 0 or total == 0:
        return -0
    return num_true/total * math.log(num_true/total,2)

def entropy_of_set(input_set):
    if len(input_set) == 0:
        return 0

    entropy_total = 0
    class_totals = {}

    # set counters of classes to zero
    for entry_class in classes:
        class_totals[entry_class] = 0

    for entry in input_set:
        for entry_class in classes:
          if entry['class'] == entry_class:
             class_totals[entry_class] += 1
     
    # calculate the net entropy by summing over the entropy of the classes    

    for entry_class in classes:
        entropy_total += entropy(class_totals[entry_class], len(input_set))

    entropy_total = abs(entropy_total)
    return entropy_total

def frequency(input_set, attribute, value):
    count = 0
    for entry in input_set:
        if entry[attribute] == value:
            count += 1
    return count / len(input_set)

def average_entropy(input_set, attribute):
    total = 0
    for value in attribute["Type"]:
         total += frequency(input_set, attribute["name"], value) * entropy_of_set(subset(input_set, attribute["name"],value))  
    return total

# Function that calculates the gain of partitioning a set
# with criterion test.

def gain(input_set, attribute): 
    return entropy_of_set(input_set) - average_entropy(input_set, attribute)

# Function that encapsulates the main logic of constructing the decision tree.
# Will need to add some recursive component

def induce(input_set):
    set_entropy = entropy_of_set(input_set)
    
    # if the set is not homogenous

    if set_entropy < 1:
        for attribute in attributes:
            print("Gain for " + attribute["name"] + " = " + str(gain(input_set, attribute)))


if __name__ == '__main__':
    parse_args()
    parse_spec_file(args.spec)
    parse_data_file(args.dat)
    partition_sets()
    induce(training_set)
    write_output()
